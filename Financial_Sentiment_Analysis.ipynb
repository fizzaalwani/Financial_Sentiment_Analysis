{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "MF6q_H7WCPpH"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data=pd.read_csv('Sentiment_Stock_data (1).csv')"
      ],
      "metadata": {
        "id": "HvuiXE7HCjbJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "M3zvqCt5ClzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.info()"
      ],
      "metadata": {
        "id": "rCa58csRCnmm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(data.loc[data['Sentence'] == ''].index, inplace=True)\n",
        "# data[data['Sentence'] == '']"
      ],
      "metadata": {
        "id": "1WoB_Iz1yAvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.shape"
      ],
      "metadata": {
        "id": "FX99NKdyCrvh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.isnull().sum()"
      ],
      "metadata": {
        "id": "8f8BiERtCt0r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.dropna(axis=0, inplace=True)"
      ],
      "metadata": {
        "id": "KSeT3XIzCwOb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.duplicated().sum()"
      ],
      "metadata": {
        "id": "URR78bA_CyQC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.drop(columns=['Unnamed: 0'],axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "bjCAp9UGC0SK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove punctuations\n",
        "import string\n",
        "def remove_punctuations(text):\n",
        "    filtered=''\n",
        "    for i in text:\n",
        "        if i not in string.punctuation:\n",
        "            filtered +=i\n",
        "    return filtered\n",
        "data['Sentence']=data['Sentence'].apply(remove_punctuations)"
      ],
      "metadata": {
        "id": "VsYe4_-qC2Uw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#convert to lowercase\n",
        "data['Sentence']=data['Sentence'].apply(lambda x:x.lower())"
      ],
      "metadata": {
        "id": "YhX-DbXrC48N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove emojis\n",
        "def remove_emojis(text):\n",
        "    filtered=''\n",
        "    for i in text:\n",
        "        if i.isascii():\n",
        "            filtered +=i\n",
        "    return filtered\n",
        "data['Sentence']=data['Sentence'].apply(remove_emojis)"
      ],
      "metadata": {
        "id": "QX97-2xGC7QD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#remove stopwords\n",
        "# import nltk\n",
        "# from nltk.corpus import stopwords\n",
        "# from nltk.stem import WordNetLemmatizer\n",
        "# stopwords=set(stopwords.words('english'))\n",
        "# def tokenize(text):\n",
        "#     words=text.split()\n",
        "#     cleaned=[]\n",
        "#     for i in words:\n",
        "#         if i not in stopwords:\n",
        "#             cleaned.append(i)\n",
        "#     return ' '.join(cleaned)\n",
        "# data['Sentence']=data['Sentence'].apply(tokenize)\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# Download resources (run once)\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Now load stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def tokenize(text):\n",
        "    words = text.split()\n",
        "    cleaned = []\n",
        "    for w in words:\n",
        "        if w.lower() not in stop_words:\n",
        "            cleaned.append(w)\n",
        "    return \" \".join(cleaned)\n",
        "\n",
        "data['Sentence'] = data['Sentence'].apply(tokenize)\n"
      ],
      "metadata": {
        "id": "cWSZFFcQC9C_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "lemmatizer=WordNetLemmatizer()\n",
        "\n",
        "def lemmatize(text):\n",
        "    words=text.split()\n",
        "    lemmatized=[lemmatizer.lemmatize(word) for word in words]\n",
        "    return ' '.join(lemmatized)\n",
        "\n",
        "data['Sentence']=data['Sentence'].apply(lemmatize)\n",
        "\n",
        "print(data.head())"
      ],
      "metadata": {
        "id": "VueQfvv0C-06"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data['no of words']=data['Sentence'].apply(lambda x:len(x.split()))"
      ],
      "metadata": {
        "id": "lclNVAlcDAzq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#now convert text into vectors\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n"
      ],
      "metadata": {
        "id": "OKexWeDaDEUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.countplot(x=data['Sentiment'])\n"
      ],
      "metadata": {
        "id": "EidzBQy-DGY2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.histplot(x=data['no of words'], kde=True)\n"
      ],
      "metadata": {
        "id": "LnrVjtxbDI9K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sns.heatmap(data.corr(numeric_only=True),annot=True)"
      ],
      "metadata": {
        "id": "2463GkJDDLM2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data[data['no of words'] < 20])\n",
        "data.head(20)"
      ],
      "metadata": {
        "id": "Gln8-1ay1JCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(data['Sentence'], data['Sentiment'], test_size=0.25, random_state=42, shuffle=True)"
      ],
      "metadata": {
        "id": "cy8yCzyhDNY7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow=CountVectorizer()\n",
        "tfidf=TfidfVectorizer(\n",
        "    max_features=60000,      # reduce vocabulary\n",
        "    ngram_range=(1,3),      # add bigrams (very important for finance)\n",
        "    min_df=3,               # remove rare words\n",
        "    max_df=0.9,\n",
        ")"
      ],
      "metadata": {
        "id": "Dfy9mmh9DPWM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train_bow=bow.fit_transform(X_train)\n",
        "X_test_bow=bow.transform(X_test)\n",
        "\n",
        "X_train_tfidf=tfidf.fit_transform(X_train)\n",
        "X_test_tfidf=tfidf.transform(X_test)"
      ],
      "metadata": {
        "id": "_vOdOi8mDS25"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.svm import LinearSVC"
      ],
      "metadata": {
        "id": "I4SHVQcdDTz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models_bow={\n",
        "    'logistic_regression':LogisticRegression(max_iter=2000),\n",
        "    'naive_bayes':MultinomialNB(),\n",
        "    'SVM_bow':LinearSVC()\n",
        "}\n",
        "\n",
        "models_tfidf={\n",
        "     'logistic_regression':LogisticRegression(max_iter=2000),\n",
        "     'naive_bayes':MultinomialNB(),\n",
        "     'SVM_tfidf':LinearSVC()\n",
        "}\n"
      ],
      "metadata": {
        "id": "6ttzc5NPDXj1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_predict_bow={}\n",
        "y_predict_tfidf={}\n",
        "\n",
        "for i in models_bow:\n",
        "    models_bow[i].fit(X_train_bow, y_train)\n",
        "    y_predict_bow[i]=models_bow[i].predict(X_test_bow)\n",
        "\n",
        "for i in models_tfidf:\n",
        "    models_tfidf[i].fit(X_train_tfidf, y_train)\n",
        "    y_predict_tfidf[i]=models_tfidf[i].predict(X_test_tfidf)\n"
      ],
      "metadata": {
        "id": "A0r4XScGDZcf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
        "print(\"\\n---- TF-IDF Accuracies ----\")\n",
        "for model_name in y_predict_tfidf:\n",
        "    print(f\"{model_name}: {accuracy_score(y_test, y_predict_tfidf[model_name])*100:.2f}%\")\n",
        "    # print(f\"{model_name}: {confusion_matrix(y_test, y_predict_tfidf[model_name])*100:.2f}%\")\n",
        "\n",
        "print(\"\\n\\n---- Bag of Words Accuracies ----\")\n",
        "for model_name in y_predict_bow:\n",
        "    print(f\"{model_name}: {accuracy_score(y_test, y_predict_bow[model_name])*100:.2f}%\")\n",
        "    # print(f\"{model_name}: {confusion_matrix(y_test, y_predict_bow[model_name])*100:.2f}%\")\n",
        "\n"
      ],
      "metadata": {
        "id": "iPJ5IHTCDbQI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense,Dropout,Input\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "metadata": {
        "id": "neHTr361DlLY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ANN = Sequential([\n",
        "    Input(shape=(X_train_bow.shape[1],), sparse=True),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.2),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.1),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "ANN.compile(optimizer='adam' , loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "Zpb1knqjEYG-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "history=ANN.fit(X_train_bow, y_train, epochs=40, verbose=1 , validation_split=0.25, batch_size=122)"
      ],
      "metadata": {
        "id": "d6fZGvrrFO3x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ANN_tfidf = Sequential([\n",
        "#     Input(shape=(X_train_tfidf.shape[1],), sparse=True),\n",
        "#     Dense(128, activation='relu'),\n",
        "#     Dropout(0.3),\n",
        "#     Dense(64, activation='relu'),\n",
        "#     Dropout(0.6),\n",
        "#     Dense(32, activation='relu'),\n",
        "#     Dropout(0.4),\n",
        "#     Dense(16, activation='relu'),\n",
        "#     Dropout(0.5),\n",
        "#     Dense(1, activation='sigmoid'),\n",
        "# ])\n",
        "# ANN_tfidf = Sequential([\n",
        "#     Input(shape=(X_train_tfidf.shape[1],), sparse=True),\n",
        "#     Dense(64, activation='relu'),\n",
        "#     Dropout(0.3),\n",
        "#     Dense(1, activation='sigmoid')\n",
        "# ])\n",
        "from tensorflow.keras import regularizers\n",
        "ANN_tfidf = Sequential([\n",
        "    Input(shape=(X_train_tfidf.shape[1],), sparse=True),\n",
        "    Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "ANN_tfidf.compile(optimizer='adam' , loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "R6lrHlGnF_kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "\n",
        "es = EarlyStopping(\n",
        "    monitor='val_loss',\n",
        "    patience=2,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "history_tfidf=ANN_tfidf.fit(X_train_tfidf, y_train, epochs=20, verbose=1 , validation_data=(X_test_tfidf, y_test), batch_size=128, callbacks=[es])"
      ],
      "metadata": {
        "id": "UQu6-mllHA80"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
        "\n",
        "# ---------------------------\n",
        "# 1. Tokenize and convert to sequences\n",
        "# ---------------------------\n",
        "\n",
        "max_words = 20000       # Vocabulary size\n",
        "max_len = 100           # Maximum length of each sequence\n",
        "\n",
        "tokenizer = Tokenizer(num_words=max_words, oov_token=\"<OOV>\")\n",
        "tokenizer.fit_on_texts(data['Sentence'])\n",
        "\n",
        "X = tokenizer.texts_to_sequences(data['Sentence'])\n",
        "X = pad_sequences(X, maxlen=max_len)\n",
        "\n",
        "# y is your label column (0/1 or multi-class)\n",
        "y = data['Sentiment']\n",
        "\n",
        "# Trainâ€“test split\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "\n",
        "# ---------------------------\n",
        "# 2. Build RNN model\n",
        "# ---------------------------\n",
        "\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=max_words, output_dim=128, input_length=max_len),\n",
        "    LSTM(64, return_sequences=False),\n",
        "    Dropout(0.4),\n",
        "    Dense(32, activation='relu'),\n",
        "    Dropout(0.3),\n",
        "    Dense(1, activation='sigmoid')  # sigmoid for binary classification\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "# ---------------------------\n",
        "# 3. Train\n",
        "# ---------------------------\n",
        "\n",
        "history = model.fit(\n",
        "    X_train, y_train,\n",
        "    validation_split=0.25,\n",
        "    batch_size=32,\n",
        "    epochs=10,\n",
        "    verbose=1\n",
        ")\n"
      ],
      "metadata": {
        "id": "Ef1kZT05OQ8o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(X_test, y_test, verbose=0)\n",
        "print(\"Test Accuracy:\", accuracy)\n"
      ],
      "metadata": {
        "id": "mWZRm-eI_buz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers torch datasets accelerate\n"
      ],
      "metadata": {
        "id": "-AhX2nGPHbWd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96cee496"
      },
      "source": [
        "# Task\n",
        "Improve the performance of the existing sentiment analysis models (Logistic Regression, Naive Bayes, SVM, ANN, LSTM) on the \"Sentiment_Stock_data.csv\" dataset by analyzing their current performance with detailed metrics, enhancing text preprocessing, implementing pre-trained word embeddings, performing hyperparameter tuning, addressing class imbalance, exploring advanced neural network architectures, using ensemble methods, and employing cross-validation for robust evaluation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59813307"
      },
      "source": [
        "## Analyze Current Model Performance\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the current models (Logistic Regression, Naive Bayes, SVM, ANN, LSTM) comprehensively using metrics like precision, recall, F1-score, and confusion matrices for each class.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22d61ba5"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to evaluate each of the trained models (Logistic Regression, Naive Bayes, SVM, ANN, ANN_tfidf, and LSTM) using classification reports and confusion matrices. This involves iterating through the dictionary of models, making predictions, converting probabilities to binary labels for neural networks, and then printing the required metrics.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89613f0d"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a182daef"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous code failed because the `y_test` variable was overwritten by the LSTM's train-test split, leading to inconsistent sample sizes when evaluating models from the earlier (traditional) split. To fix this, I will re-create the `y_test` specifically for the traditional models (BoW/TF-IDF and their ANNs) using the same parameters as the original split, and use the existing global `y_test` for the LSTM model, ensuring consistent sample sizes for evaluation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "59e2a9d1"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    }
  ]
}